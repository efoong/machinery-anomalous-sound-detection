{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26726,"status":"ok","timestamp":1723966452144,"user":{"displayName":"E. Foong","userId":"14542078563796012054"},"user_tz":-480},"id":"XIv7J8_cRadq","outputId":"720b909d-6503-4c7f-e8a7-e86755dbd4ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["%pip install optuna\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import mutual_info_regression\n","from scipy.stats import entropy\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","import librosa\n","\n","import pickle\n","import optuna\n","\n","# read data from google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["def load_audio_files(base_directory, dataset='train'):\n","    categories = ['fan', 'gearbox', 'pump', 'slider', 'ToyCar', 'ToyTrain', 'valve']\n","    base_directories = [base_directory + '/' + category + '/' for category in categories]\n","    audio_files = []\n","\n","    for i, base_directory in enumerate(base_directories):\n","        category = categories[i]\n","        for root, _, files in os.walk(base_directory):\n","            for filename in files:\n","                if filename.endswith('.wav'):\n","                    filepath = os.path.join(root, filename)\n","                    audio_files.append(filepath)\n","                    audio, sr = librosa.load(filepath, sr=None)\n","                    section = filename[8:10]\n","                    if 'normal' in filename:\n","                        audio_files.append((audio, sr, category, dataset, 'normal', section))\n","                    elif 'anomaly' in filename:\n","                        audio_files.append((audio, sr, category, dataset, 'anomaly', section))\n","    return audio_files\n","\n","base_directory = '/content'\n","train_audio_files = load_audio_files(base_directory, dataset='train')"],"metadata":{"id":"_yfhWnFEU18n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrK-GejiRgAM"},"outputs":[],"source":["machine = ['fan', 'gearbox', 'pump', 'slider', 'ToyCar', 'ToyTrain']\n","# Specify the filename of the pickle file\n","# for mel-spectrogram\n","f1 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f1, 'rb') as file:\n","    train_data = pickle.load(file)\n","\n","f2 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f2, 'rb') as file:\n","    train_label = pickle.load(file)\n","\n","f3 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f3, 'rb') as file:\n","    test_data = pickle.load(file)\n","\n","f4 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f4, 'rb') as file:\n","    test_label = pickle.load(file)\n","\n","# for idnn\n","f5 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f5, 'rb') as file:\n","    y_train = pickle.load(file)\n","\n","f6 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f6, 'rb') as file:\n","    y_test = pickle.load(file)\n","\n","f7 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f7, 'rb') as file:\n","    X_train_indices = pickle.load(file)\n","\n","f8 = '/content/drive/MyDrive/path_to_file.pkl'\n","with open(f8, 'rb') as file:\n","    X_test_indices = pickle.load(file)"]},{"cell_type":"code","source":["\n","# Function to compute mutual information between original waveforms and log mel spectrograms\n","def compute_mutual_information(waveforms, log_mel_spectrograms):\n","    mi_scores = []\n","    for i in range(waveforms.shape[0]):\n","    # Average pooling of the spectrogram to match the waveform length\n","        pooled_spectrogram = log_mel_spectrograms[i].mean(axis=0)\n","        pooled_spectrogram = pooled_spectrogram.flatten()\n","        if len(pooled_spectrogram) != len(waveforms[i]):\n","    # Downsample waveform to match pooled spectrogram length\n","            downsampled_waveform = librosa.resample(waveforms[i], orig_sr=len(waveforms[i]), target_sr=len(pooled_spectrogram))\n","        else:\n","            downsampled_waveform = waveforms[i]\n","\n","        mi = mutual_info_regression(pooled_spectrogram.reshape(-1, 1), downsampled_waveform)\n","        mi_scores.append(mi.mean())\n","    return np.mean(mi_scores)\n","\n","\n","# Function to compute Shannon entropy of the original waveforms and log mel spectrograms\n","def compute_entropy(data):\n","    entropies = []\n","    for sample in data:\n","        # Normalize the sample to avoid log(0)\n","        sample = sample - np.min(sample)\n","        sample = sample / np.max(sample)\n","        ent = entropy(sample)\n","        entropies.append(ent)\n","    return np.mean(entropies)\n","\n","log_mel_spectrograms = np.array(features_list)\n","# Compute mutual information\n","mi_waveform_vs_spectrogram = compute_mutual_information(waveforms, log_mel_spectrograms)\n","print(f\"Average Mutual Information: {mi_waveform_vs_spectrogram}\")\n","\n","# Compute entropy for original waveforms and spectrograms\n","entropy_waveforms = compute_entropy(waveforms)\n","entropy_spectrograms = compute_entropy(log_mel_spectrograms.reshape(B, -1))\n","\n","print(f\"Average Entropy of Waveforms: {entropy_waveforms}\")\n","print(f\"Average Entropy of Spectrograms: {entropy_spectrograms}\")\n"],"metadata":{"id":"9Hs71utu7V3I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723777344586,"user_tz":-480,"elapsed":925,"user":{"displayName":"E. Foong","userId":"14542078563796012054"}},"outputId":"7b5653bb-418b-4ce3-b6fc-b0230a3f901d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_e6UqIKvRgB3"},"outputs":[],"source":["# read the labels\n","# train_labels_df = pd.DataFrame(train_label, columns=['machine_type', 'source', 'normal_anomaly', 'section'])\n","test_labels_df = pd.DataFrame(test_label, columns=['machine_type', 'source', 'normal_anomaly', 'section'])\n","\n","section = test_labels_df['section'].unique()\n","source = test_labels_df['source'].unique()\n","machine_type = test_labels_df['machine_type'].unique()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"fMb46mFuT8uh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioope6UIKcLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723954384173,"user_tz":-480,"elapsed":24,"user":{"displayName":"E. Foong","userId":"14542078563796012054"}},"outputId":"78a67625-d045-4147-d0f3-247d0e0881d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['valve'], dtype=object)"]},"metadata":{},"execution_count":9}],"source":["# train_labels_df[train_labels_df['machine_type'] == 'fan']\n","# test_labels_df\n","machine_type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RE9N2g2pRgDy"},"outputs":[],"source":["def get_train_data_machine(machine_type):\n","\n","    train_indices = train_labels_df[train_labels_df['machine_type'] == machine_type].index\n","    test_indices = test_labels_df[test_labels_df['machine_type'] == machine_type].index\n","\n","    train_data_filtered = train_data[train_indices]\n","    test_data_filtered = test_data[test_indices]\n","    test_labels_filtered = test_labels_df.loc[test_indices, 'normal_anomaly'].values\n","    test_labels_filtered = np.array([1 if x == 'anomaly' else 0 for x in test_labels_filtered])\n","    test_category_filtered = test_labels_df.loc[test_indices, ['source', 'section']].reset_index().drop(columns='index')\n","\n","    return train_data_filtered, test_data_filtered, test_labels_filtered, test_category_filtered\n","\n","def get_labels_machine_idnn(machine_type):\n","\n","    train_indices = train_labels_df[train_labels_df['machine_type'] == machine_type].index\n","    test_indices = test_labels_df[test_labels_df['machine_type'] == machine_type].index\n","\n","    test_labels_filtered = test_labels_df.loc[test_indices, 'normal_anomaly'].values\n","    test_labels_filtered = np.array([1 if x == 'anomaly' else 0 for x in test_labels_filtered])\n","    test_category_filtered = test_labels_df.loc[test_indices, ['source', 'section']].reset_index().drop(columns='index')\n","\n","    return test_labels_filtered, test_category_filtered\n","\n","def get_label(train_data, test_data, test_labels_df):\n","\n","    train_data_filtered = np.array(train_data)\n","    test_data_filtered = np.array(test_data)\n","    test_labels_filtered = test_labels_df['normal_anomaly'].values\n","    test_labels_filtered = np.array([1 if x == 'anomaly' else 0 for x in test_labels_filtered])\n","    test_category_filtered = test_labels_df[['source', 'section']]\n","\n","    return train_data_filtered, test_data_filtered, test_labels_filtered, test_category_filtered\n","\n","\n"]},{"cell_type":"code","source":["# for idnn only\n","def extract_patches_with_tracking(data, patch_size=3):\n","    patches = []\n","    labels = []\n","    indices = []\n","    for b in range(data.shape[0]):\n","        sample = data[b]\n","        for i in range(1, sample.shape[0]-1):\n","            for j in range(1, sample.shape[1]-1):\n","                patch = sample[i-1:i+2, j-1:j+2]\n","                patches.append(np.delete(patch.flatten(), 4))  # Remove the center element\n","                labels.append(patch[1, 1])  # Center element as the label\n","                indices.append((b, i, j))  # Track (spectrogram index, row, col)\n","    return np.array(patches), np.array(labels), indices\n","\n","X_train, Y_train, indices_train = extract_patches_with_tracking(train_data)\n","X_train = X_train.reshape(-1, 8)\n","X_test, Y_test, indices_test = extract_patches_with_tracking(test_data)\n","X_test = X_test.reshape(-1, 8)\n","\n","def get_label(test_labels_df):\n","\n","    test_labels_filtered = test_labels_df['normal_anomaly'].values\n","    test_labels_filtered = np.array([1 if x == 'anomaly' else 0 for x in test_labels_filtered])\n","    test_category_filtered = test_labels_df[['source', 'section']]\n","\n","    return test_labels_filtered, test_category_filtered"],"metadata":{"id":"imioKwPftVRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDzkxtgFWSga"},"outputs":[],"source":["# define auto encoder model\n","def create_autoencoder(data, units_layer_1, units_layer_2, units_layer_3, units_layer_4, latent_dim):\n","\n","    input_shape = (data.shape[1], data.shape[2])\n","    # Create the input layer\n","    input_layer = layers.Input(shape=input_shape)\n","\n","    # Flatten the input if you are using dense layers for encoding\n","    flattened_input = layers.Flatten()(input_layer)\n","\n","    #Encoder block\n","    x = layers.Dense(units_layer_1, activation=\"relu\")(flattened_input)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_2, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_3, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_4, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    #Latent space\n","    x = layers.Dense(latent_dim, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    #Decoder block\n","    x = layers.Dense(units_layer_4, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_3, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_2, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Dense(units_layer_1, activation=\"relu\")(x)\n","    x = layers.BatchNormalization()(x)\n","    #Output layer\n","    x = layers.Dense(input_shape[0]*input_shape[1], activation=None)(x)\n","    output_layer = layers.Reshape(input_shape)(x)\n","\n","    # Create and return the model\n","    autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n","\n","    return autoencoder\n","\n","# Data Preparation\n","def prepare_dataloader(data, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices(data)\n","    dataset = dataset.batch(batch_size).shuffle(buffer_size=len(data))\n","    return dataset\n","\n","# Training Function\n","def train_model(model, dataloader, epochs, optimizer, loss_fn):\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch+1}/{epochs}')\n","        for batch in dataloader:\n","            with tf.GradientTape() as tape:\n","                reconstruction = model(batch, training=True)\n","                loss = loss_fn(batch, reconstruction)\n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        print(f'Loss: {loss.numpy()}')\n","\n","\n","def objective(trial, data_train, data_test):\n","    # Suggest the number of units for each hidden layer\n","    units_layer_1 = trial.suggest_int('units_layer_1', 320, 512)\n","    units_layer_2 = trial.suggest_int('units_layer_2', 256, 320)\n","    units_layer_3 = trial.suggest_int('units_layer_3', 128, 256)\n","    units_layer_4 = trial.suggest_int('units_layer_4', 64, 128)\n","    latent_dim = trial.suggest_int('latent_dim', 8, 32)\n","\n","    # Suggest the learning rate and batch size\n","    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n","    batch_size = trial.suggest_categorical('batch_size', [128, 256])\n","\n","    # Suggest the number of epochs\n","    epochs = trial.suggest_int('epochs', 100, 300)\n","\n","    # Create the autoencoder model with the suggested parameters\n","    autoencoder = create_autoencoder(data_train, units_layer_1, units_layer_2, units_layer_3, units_layer_4, latent_dim)\n","\n","    # Define the optimizer and loss function\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","    loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","    # Prepare the data loader\n","    train_loader = prepare_dataloader(data_train, batch_size)\n","\n","    # Train the model\n","    train_model(autoencoder, train_loader, epochs, optimizer, loss_fn)\n","\n","    # Evaluate on validation set\n","    reconstruction = autoencoder.predict(data_test)\n","    loss = loss_fn(data_test, reconstruction).numpy()\n","\n","    return loss\n","\n","def calculate_ae_metrics(mse, test_labels_filtered, test_category_filtered):\n","    results = []\n","\n","    for s in section:\n","        for so in source:\n","            idx = test_category_filtered[ (test_category_filtered['source'] == so ) &\n","                                        (test_category_filtered['section'] == s)\n","                                        ].index\n","\n","            mse_subset = mse[idx]\n","            test_labels_subset = test_labels_filtered[idx]\n","\n","            anomaly_percent_subset = np.sum(test_labels_subset) / test_labels_subset.shape[0] * 100\n","            print(f'the percentage of anomaly from section {s} of {so} is {anomaly_percent_subset:.2f} %')\n","\n","            threshold_subset = np.percentile(mse_subset, 100-anomaly_percent_subset)\n","            # threshold_subset = np.percentile(mse_subset, 55)\n","            anomalies_subset = np.array([1 if x > threshold_subset else 0 for x in mse_subset])\n","\n","            auc = roc_auc_score(test_labels_subset, mse_subset)\n","            p_auc = roc_auc_score(test_labels_subset, mse_subset, max_fpr=0.1)\n","            f1 = f1_score(test_labels_subset, anomalies_subset)\n","\n","            results.append([s, so, anomaly_percent_subset, threshold_subset, auc, p_auc, f1])\n","\n","    return results\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T03W4NjRgF9"},"outputs":[],"source":["# define the objective functions for each model for hyper parameter tuning\n","\n","def create_cnn_autoencoder(data, filter1, filter2, filter3, kernel_size1, kernel_size2, kernel_size3):\n","    input_shape = (data.shape[1], data.shape[2], 1)\n","    input_layer = layers.Input(shape=input_shape)  # (B, 28, 640, 1)\n","\n","    # Encoder\n","    x = layers.Conv2D(filter1, kernel_size1, activation='relu', padding='same')(input_layer)  # (B, 28, 640, 1) -> (B, 28, 640, 64)\n","    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # (B, 14, 320, 64)\n","    x = layers.Conv2D(filter2, kernel_size2, activation='relu', padding='same')(x)  # (B, 14, 320, 64) -> (B, 14, 320, 32)\n","    x = layers.MaxPooling2D((2, 2), padding='same')(x)  # (B, 7, 160, 32)\n","    x = layers.Conv2D(filter3, kernel_size3, activation='relu', padding='same')(x)  # (B,  7, 160, 32) -> (B,  7, 160, 16)\n","    encoded = layers.MaxPooling2D((1, 2), padding='same')(x)  # (B,  4, 80, 16)\n","\n","    # Decoder\n","    x = layers.Conv2DTranspose(filter3, kernel_size3, activation='relu', padding='same')(encoded)\n","    x = layers.UpSampling2D((1, 2))(x)\n","    x = layers.Conv2DTranspose(filter2, kernel_size2, activation='relu', padding='same')(x)\n","    x = layers.UpSampling2D((2, 2))(x)\n","    x = layers.Conv2DTranspose(filter1, kernel_size1, activation='relu', padding='same')(x)\n","    x = layers.UpSampling2D((2, 2))(x)\n","    decoded = layers.Conv2D(1, kernel_size1, activation='sigmoid', padding='same')(x)\n","\n","    cnn = models.Model(inputs=input_layer, outputs=decoded)\n","\n","    return cnn\n","\n","\n","def prepare_dataloader(data, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices(data)\n","    dataset = dataset.batch(batch_size).shuffle(buffer_size=len(data))\n","    return dataset\n","\n","# Training Function\n","def train_model(model, dataloader, epochs, optimizer, loss_fn):\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch+1}/{epochs}')\n","        for batch in dataloader:\n","            with tf.GradientTape() as tape:\n","                reconstruction = model(batch, training=True)\n","                loss = loss_fn(batch, reconstruction)\n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        print(f'Loss: {loss.numpy()}')\n","\n","# def objective_cnn(trial, data_train, data_test, data_train_expanded, data_test_expanded):\n","def objective_cnn(trial, data_train, data_test):\n","\n","    filter1 = trial.suggest_int('filter1', 64, 128)\n","    filter2 = trial.suggest_int('filter2', 16, 64)\n","    filter3 = trial.suggest_int('filter3', 4, 16)\n","    kernel_size1 = trial.suggest_int('kernel_size1', 3, 5)\n","    kernel_size2 = trial.suggest_int('kernel_size2', 3, 5)\n","    kernel_size3 = trial.suggest_int('kernel_size3', 3, 5)\n","\n","    # Suggest the learning rate and batch size\n","    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n","    batch_size = trial.suggest_categorical('batch_size', [64, 96])\n","\n","    # Suggest the number of epochs\n","    epochs = trial.suggest_int('epochs', 100, 300)\n","\n","    # Create the autoencoder model with the suggested parameters\n","    cnn = create_cnn_autoencoder(data_train, filter1, filter2, filter3, kernel_size1, kernel_size2, kernel_size3)\n","\n","    # Define the optimizer and loss function\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","    loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","    # Prepare the data loader\n","    train_loader = prepare_dataloader(data_train, batch_size)\n","\n","\n","    # Train the model\n","    train_model(cnn, train_loader, epochs, optimizer, loss_fn)\n","\n","\n","    reconstruction = cnn.predict(data_test)\n","    loss = np.mean(np.mean(np.square(reconstruction - data_test), axis=(1, 2, 3)))\n","\n","    print('val_loss=', loss)\n","    return loss\n","\n","def calculate_cnn_metrics(mse, test_labels_filtered, test_category_filtered):\n","    results = []\n","    for s in section:\n","        for so in source:\n","            idx = test_category_filtered[ (test_category_filtered['source'] == so ) &\n","                                        (test_category_filtered['section'] == s)\n","                                        ].index\n","\n","            mse_subset = mse[idx]\n","            test_labels_subset = test_labels_filtered[idx]\n","\n","            anomaly_percent_subset = np.sum(test_labels_subset) / test_labels_subset.shape[0] * 100\n","            print(f'the percentage of anomaly from section {s} of {so} is {anomaly_percent_subset:.2f} %')\n","\n","            threshold_subset = np.percentile(mse_subset, 100-anomaly_percent_subset)\n","            anomalies_subset = np.array([1 if x > threshold_subset else 0 for x in mse_subset])\n","\n","            auc = roc_auc_score(test_labels_subset, mse_subset)\n","            p_auc = roc_auc_score(test_labels_subset, mse_subset, max_fpr=0.1)\n","            f1 = f1_score(test_labels_subset, anomalies_subset)\n","\n","            results.append([s, so, anomaly_percent_subset, threshold_subset, auc, p_auc, f1])\n","\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abYK2Z90Uk-J"},"outputs":[],"source":["# # define idnn model\n","\n","def create_fc_idnn(data, n_layers, units, dropout_rate, learning_rate):\n","    model = models.Sequential()\n","    model.add(layers.InputLayer(input_shape=(data.shape[1],)))\n","\n","    for _ in range(n_layers):\n","        model.add(layers.Dense(units, activation='relu'))\n","        model.add(layers.Dropout(dropout_rate))\n","\n","    model.add(layers.Dense(1))  # Output layer for regression\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse')\n","\n","    return model\n","\n","def create_idnn_model(trial):\n","    # Suggest hyperparameters\n","    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n","    units = trial.suggest_int(\"units\", 8, 64)\n","    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n","    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n","\n","    model = models.Sequential()\n","    model.add(layers.InputLayer(input_shape=(8,)))\n","\n","    for _ in range(n_layers):\n","        model.add(layers.Dense(units, activation='relu'))\n","        model.add(layers.Dropout(dropout_rate))\n","\n","    model.add(layers.Dense(1))  # Output layer for regression\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse')\n","\n","    return model\n","\n","\n","def objective_idnn(trial, x_train, y_train, x_test, y_test, idx_test):\n","    model = create_idnn_model(trial)\n","\n","    # Suggest hyperparameters for training\n","    batch_size = trial.suggest_int(\"batch_size\", 4096, 8192)\n","    epochs = trial.suggest_int(\"epochs\", 5, 25)\n","\n","    # Train the model\n","    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=1)\n","\n","    # Predict and calculate element-wise squared error for each patch\n","    predictions = model.predict(x_test, batch_size=batch_size)\n","    element_wise_squared_error = (predictions.reshape(y_test.shape) - y_test) ** 2\n","\n","    # Aggregate reconstruction error by spectrogram\n","    error_dict = {}\n","    for i, (b, row, col) in enumerate(idx_test):\n","        if b not in error_dict:\n","            error_dict[b] = []\n","        error_dict[b].append(element_wise_squared_error[i])\n","\n","    # Calculate mean squared error for each spectrogram\n","    mean_errors = {b: np.mean(errors) for b, errors in error_dict.items()}\n","    mse = np.mean(np.array(list(mean_errors.values())))\n","\n","    return mse\n","\n","def calculate_idnn_metrics(mean_errors, test_labels_filtered, test_category_filtered):\n","    results = []\n","    for s in section:\n","        for so in source:\n","            idx = test_category_filtered[ (test_category_filtered['source'] == so ) &\n","                                        (test_category_filtered['section'] == s)\n","                                        ].index\n","\n","\n","            print(len(idx))\n","            print(type(mean_errors))\n","            print(len(mean_errors))\n","            mse_subset = mean_errors[idx]\n","            test_labels_subset = test_labels_filtered[idx]\n","\n","\n","            anomaly_percent_subset = np.sum(test_labels_subset) / test_labels_subset.shape[0] * 100\n","\n","            print(f'the percentage of anomaly from section {s} of {so} is {anomaly_percent_subset:.2f} %')\n","\n","            threshold_subset = np.percentile(mse_subset, 100-anomaly_percent_subset)\n","            anomalies_subset = np.array([1 if x > threshold_subset else 0 for x in mse_subset])\n","\n","            auc = roc_auc_score(test_labels_subset, mse_subset)\n","            p_auc = roc_auc_score(test_labels_subset, mse_subset, max_fpr=0.1)\n","            f1 = f1_score(test_labels_subset, anomalies_subset)\n","\n","            results.append([s, so, anomaly_percent_subset, threshold_subset, auc, p_auc, f1])\n","\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBsFjCwLRgJy"},"outputs":[],"source":["def train_and_evaluate(feat, machine_type, model, no_trials):\n","\n","    from sklearn.metrics import roc_auc_score, f1_score\n","    if feat == 'spectro':\n","        # Filter data for the given machine type\n","        if model == 'idnn':\n","            test_labels_filtered, test_category_filtered = get_labels_machine_idnn(machine_type)\n","            # print(len(test_labels_filtered))\n","            # print(test_category_filtered)\n","        else:\n","            train_data_filtered, test_data_filtered, test_labels_filtered, test_category_filtered = get_train_data_machine(machine_type)\n","    elif feat == 'emb':\n","        test_labels_filtered, test_category_filtered = get_label(test_labels_df)\n","        # train_data_filtered, test_data_filtered, test_labels_filtered, test_category_filtered = get_label(train_data, test_data, test_labels_df)\n","\n","    # set the number of trials\n","    trials = no_trials\n","\n","    if model == 'ae':\n","        study = optuna.create_study(direction='minimize')\n","        # study.optimize(objective, n_trials=trials)\n","        study.optimize(lambda trial: objective(trial, train_data_filtered, test_data_filtered), n_trials=trials)\n","        best_params = study.best_params\n","        print(best_params)\n","\n","        # Train Final Model with Best Hyperparameters\n","        latent_dim = best_params['latent_dim']\n","        lr = best_params['lr']\n","        batch_size = best_params['batch_size']\n","        epochs = best_params['epochs']\n","        units_layer_1 = best_params['units_layer_1']\n","        units_layer_2 = best_params['units_layer_2']\n","        units_layer_3 = best_params['units_layer_3']\n","        units_layer_4 = best_params['units_layer_4']\n","\n","\n","        autoencoder = create_autoencoder(train_data_filtered, units_layer_1, units_layer_2, units_layer_3, units_layer_4, latent_dim)\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","        loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","        train_loader = prepare_dataloader(train_data_filtered, batch_size)\n","        train_model(autoencoder, train_loader, epochs, optimizer, loss_fn)\n","\n","        autoencoder.save(f'/content/drive/MyDrive/Data Science/Final Project/models/{feat}_ae_{machine_type}.h5')\n","\n","        # Predict, calculate reconstruction error and calculate metrics\n","        reconstruction = autoencoder.predict(test_data_filtered)\n","        mse = np.mean(np.square(reconstruction - test_data_filtered), axis=(1, 2))\n","        results = calculate_ae_metrics(mse, test_labels_filtered, test_category_filtered)\n","\n","    elif model == 'cnn':\n","\n","        train_data_filtered_expanded = np.expand_dims(train_data_filtered, axis=-1)\n","        test_data_filtered_expanded = np.expand_dims(test_data_filtered, axis=-1)\n","        print(train_data_filtered_expanded.shape)\n","        print(test_data_filtered_expanded.shape)\n","\n","        study = optuna.create_study(direction='minimize')\n","        study.optimize(lambda trial: objective_cnn(trial, train_data_filtered_expanded, test_data_filtered_expanded), n_trials=trials)\n","\n","        best_params = study.best_params\n","        print(best_params)\n","\n","        # Train Final Model with Best Hyperparameters\n","        filter1 = best_params['filter1']\n","        filter2 = best_params['filter2']\n","        filter3 = best_params['filter3']\n","        kernel_size1 = best_params['kernel_size1']\n","        kernel_size2 = best_params['kernel_size2']\n","        kernel_size3 = best_params['kernel_size3']\n","        lr = best_params['lr']\n","        batch_size = best_params['batch_size']\n","        epochs = best_params['epochs']\n","\n","        cnn = create_cnn_autoencoder(train_data_filtered_expanded, filter1, filter2, filter3, kernel_size1, kernel_size2, kernel_size3)\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","        loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","        train_loader = prepare_dataloader(train_data_filtered_expanded, batch_size)\n","        train_model(cnn, train_loader, epochs, optimizer, loss_fn)\n","        cnn.save(f'/content/drive/MyDrive/Data Science/Final Project/models/{feat}_cnn_{machine_type}.h5')\n","\n","        # Predict and calculate reconstruction error\n","        reconstruction = cnn.predict(test_data_filtered_expanded)\n","        mse = np.mean(np.square(reconstruction - test_data_filtered_expanded), axis=(1, 2, 3))\n","\n","        results = calculate_cnn_metrics(mse, test_labels_filtered, test_category_filtered)\n","\n","\n","    elif model == 'idnn':\n","        study = optuna.create_study(direction=\"minimize\")\n","        study.optimize(lambda trial: objective_idnn(trial, train_data, y_train, test_data, y_test, X_test_indices), n_trials=trials)\n","\n","        best_params = study.best_params\n","        print(best_params)\n","\n","        # Train Final Model with Best Hyperparameters\n","        lr = best_params['learning_rate']\n","        batch_size = best_params['batch_size']\n","        epochs = best_params['epochs']\n","        units = best_params['units']\n","        n_layers = best_params['n_layers']\n","        dropout_rate = best_params['dropout_rate']\n","\n","        idnn = create_fc_idnn(train_data, n_layers, units, dropout_rate, lr)\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","        loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","        history = idnn.fit(train_data, y_train, epochs=epochs, batch_size=batch_size, validation_data=(test_data, y_test), verbose=1)\n","\n","        idnn.save(f'/content/drive/MyDrive/Data Science/Final Project/models/{feat}_idnn_{machine_type}.h5')\n","\n","        # Predict and calculate element-wise squared error for each patch\n","        predictions = idnn.predict(test_data, batch_size=batch_size)\n","        element_wise_squared_error = (predictions.reshape(y_test.shape) - y_test) ** 2\n","\n","        # Aggregate reconstruction error by spectrogram\n","        error_dict = {}\n","        for i, (b, row, col) in enumerate(X_test_indices):\n","            if b not in error_dict:\n","                error_dict[b] = []\n","            error_dict[b].append(element_wise_squared_error[i])\n","\n","        # Calculate mean squared error for each spectrogram\n","        mean_errors = {b: np.mean(errors) for b, errors in error_dict.items()}\n","        mse = np.array(list(mean_errors.values()))\n","\n","        # calculate the metrics for each section and domain\n","        results = calculate_idnn_metrics(mse, test_labels_filtered, test_category_filtered)\n","\n","    else:\n","        print(f'ValueError: Model {model} is not available!')\n","\n","\n","    print(f\"Machine Type: {machine_type} with {model} + {feat}, metrics calculation completed!\" )\n","\n","\n","    # export the results\n","    df = pd.DataFrame(results, columns=['section', 'source', 'percentile',\n","                                        'threshold', 'auc', 'p_auc', 'f1'])\n","    df.to_csv(f'/content/drive/MyDrive/Data Science/Final Project/data/results_{feat}_{model}_{machine_type}.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iN_H7dCCV9hU","outputId":"606f4afe-289c-465f-dffd-07395217b2c3","executionInfo":{"status":"ok","timestamp":1723956041985,"user_tz":-480,"elapsed":1135779,"user":{"displayName":"E. Foong","userId":"14542078563796012054"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3ms/step - loss: 0.8324 - val_loss: 0.1064\n","Epoch 2/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0815 - val_loss: 0.0563\n","Epoch 3/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0579 - val_loss: 0.0486\n","Epoch 4/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0547 - val_loss: 0.0524\n","Epoch 5/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0535 - val_loss: 0.0492\n","Epoch 6/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0533 - val_loss: 0.0470\n","Epoch 7/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0532 - val_loss: 0.0488\n","Epoch 8/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0531 - val_loss: 0.0498\n","Epoch 9/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0524 - val_loss: 0.0467\n","Epoch 10/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0523 - val_loss: 0.0484\n","Epoch 11/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0519 - val_loss: 0.0378\n","Epoch 12/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0407 - val_loss: 0.0371\n","Epoch 13/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0405 - val_loss: 0.0318\n","Epoch 14/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0404 - val_loss: 0.0376\n","Epoch 15/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0403 - val_loss: 0.0337\n","Epoch 16/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0403 - val_loss: 0.0368\n","Epoch 17/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0397 - val_loss: 0.0399\n","Epoch 18/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0397 - val_loss: 0.0399\n","Epoch 19/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0397 - val_loss: 0.0413\n","Epoch 20/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0383\n","Epoch 21/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0379\n","Epoch 22/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0397\n","Epoch 23/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0401\n","Epoch 24/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0386\n","Epoch 25/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0388\n","Epoch 26/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0404\n","Epoch 27/27\n","\u001b[1m16209/16209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0448\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m6464/6464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 00 of source_test is 50.00 %\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 00 of target_test is 50.00 %\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 01 of source_test is 50.00 %\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 01 of target_test is 50.00 %\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 02 of source_test is 50.00 %\n","200\n","<class 'numpy.ndarray'>\n","1200\n","the percentage of anomaly from section 02 of target_test is 50.00 %\n","Machine Type: valve with idnn + emb, metrics calculation completed!\n"]}],"source":["# for machine in machine_type:\n","for machine in machine_type:\n","    train_and_evaluate('emb', machine, 'idnn', 50)"]},{"cell_type":"code","source":[],"metadata":{"id":"0JvrDuJDucZy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1S8bL5dAtRQaBcc8Qoys4Luv_kUinLwBw","timestamp":1726899151958}],"gpuType":"L4","authorship_tag":"ABX9TyMTsGdWK7VvK/VJjda8FYf8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}