{"cells":[{"cell_type":"code","execution_count":null,"id":"a7ed50c0-a59c-46bf-8e51-389251340d73","metadata":{"id":"a7ed50c0-a59c-46bf-8e51-389251340d73"},"outputs":[],"source":["%pip install openl3\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","import os\n","\n","from scipy.signal import spectrogram\n","from sklearn.metrics import accuracy_score\n","\n","import openl3\n","import librosa\n","import librosa.display"]},{"cell_type":"code","execution_count":null,"id":"14377036-2d89-4835-9093-7e433c372cba","metadata":{"id":"14377036-2d89-4835-9093-7e433c372cba"},"outputs":[],"source":["# reading audio files from drive\n","machine = 'fan', 'gearbox', 'pump', 'slider', 'ToyCar', 'ToyTrain', 'valve']\n","\n","def load_audio_files(base_directory, dataset='train'):\n","    categories = ['fan', 'gearbox', 'pump', 'slider', 'ToyCar', 'ToyTrain', 'valve']\n","    base_directories = [base_directory + '/' + category + '/' + dataset for category in categories]\n","    audio_files = []\n","\n","    for i, base_directory in enumerate(base_directories):\n","        category = categories[i]\n","        for root, _, files in os.walk(base_directory):\n","            for filename in files:\n","                if filename.endswith('.wav'):\n","                    filepath = os.path.join(root, filename)\n","                    audio_files.append(filepath)\n","                    audio, sr = librosa.load(filepath, sr=None)\n","                    section = filename[8:10]\n","                    if 'normal' in filename:\n","                        audio_files.append((audio, sr, category, dataset, 'normal', section))\n","                        audio_files.append(base_directories)\n","                    elif 'anomaly' in filename:\n","                        audio_files.append((audio, sr, category, dataset, 'anomaly', section))\n","                        audio_files.append(base_directories)\n","    return audio_files\n","\n","base_directory = \"/path/to/dataset\"\n","train_audio_files = load_audio_files(base_directory, dataset='train')\n","# train_data_waveform = [x[0] for x in train_audio_files]"]},{"cell_type":"code","execution_count":null,"id":"1c247c35-f13a-42cd-b65f-134cb927891c","metadata":{"id":"1c247c35-f13a-42cd-b65f-134cb927891c"},"outputs":[],"source":["source_test_audio_files = load_audio_files(base_directory, dataset='source_test')\n","target_test_audio_files = load_audio_files(base_directory, dataset='target_test')\n","\n","test_audio_files = source_test_audio_files + target_test_audio_files\n","\n","# open a file, where you stored the pickled data\n","# test_data_waveform = [x[0] for x in test_audio_files]"]},{"cell_type":"code","execution_count":null,"id":"87eb1b34-0780-47db-868d-f4913236cbd2","metadata":{"id":"87eb1b34-0780-47db-868d-f4913236cbd2"},"outputs":[],"source":["import pickle\n","\n","f1 = \"/path/to/dataset\"\n","with open(f1, 'rb') as file:\n","    train_data = pickle.load(file)\n","\n","f2 = \"/path/to/dataset\"\n","with open(f2, 'rb') as file:\n","    train_label = pickle.load(file)\n","\n","# get the unique pretrained embeddings\n","f3 = \"/path/to/dataset\"\n","with open(f3, 'rb') as file:\n","    train_emb = pickle.load(file)\n","\n","# deduplicate\n","unique = []\n","for sub_a in train_emb:\n","    if not any([np.array_equal(i, sub_a) for i in unique]):\n","        unique.append(sub_a)\n","\n","pretrained = np.array(unique)"]},{"cell_type":"code","execution_count":null,"id":"6eb21c25-f521-4c1b-b79a-57191501ef02","metadata":{"id":"6eb21c25-f521-4c1b-b79a-57191501ef02"},"outputs":[],"source":["train_data = np.array([x[0] for x in train_audio_files])\n","train_machine = [x[2] for x in train_audio_files]\n","train_source = [x[3] for x in train_audio_files]\n","train_label = [x[4] for x in train_audio_files]\n","train_section = [x[5] for x in train_audio_files]\n","\n","train_labels = [(a, b, c, d) for a, b, c, d in zip(train_machine, train_source, train_label, train_section)]"]},{"cell_type":"code","execution_count":null,"id":"39e25605-4134-45e3-99a9-dffb659f6f04","metadata":{"id":"39e25605-4134-45e3-99a9-dffb659f6f04"},"outputs":[],"source":["# Frame size and hop length in samples\n","\n","def extract_mel_spectrogram(signal, sr, frame, hop ):\n","    frame_size = int(0.064 * sr)  # 64 ms frame size\n","    hop_length = int(0.032 * sr)  # 50% hop length\n","\n","    # Extract the mel spectrogram\n","    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=frame_size, hop_length=hop_length, n_mels=128)\n","\n","    # Convert to log-mel spectrogram\n","    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","\n","    return log_mel_spectrogram\n","\n","# Concatenate 5 consecutive frames\n","def concatenate_frames(log_mel_spectrogram, P=5):\n","    num_frames = log_mel_spectrogram.shape[1]\n","    concatenated_features = []\n","\n","    for i in range(num_frames - P + 1):\n","        feature_vector = log_mel_spectrogram[:, i:i + P].flatten()\n","        concatenated_features.append(feature_vector)\n","\n","    return np.array(concatenated_features)\n"]},{"cell_type":"code","execution_count":null,"id":"a7dedc01-8887-48f8-af8f-5b94041e93a3","metadata":{"id":"a7dedc01-8887-48f8-af8f-5b94041e93a3"},"outputs":[],"source":["def pickle_data(feat_list, label_lst, filename1 ):\n","    import pickle\n","    features_arr = np.array(feat_list)\n","    labels = label_lst\n","    filename_data = f'{filename1}_data.pkl'\n","    filename_label = f'{filename1}_label.pkl'\n","    with open(filename_data, 'wb') as file1:\n","        pickle.dump(features_arr, file1)\n","    with open(filename_label, 'wb') as file2:\n","        pickle.dump(labels, file2)\n"]},{"cell_type":"code","execution_count":null,"id":"cf101a0c-9cab-4570-8cdf-e7e519a8aac9","metadata":{"id":"cf101a0c-9cab-4570-8cdf-e7e519a8aac9"},"outputs":[],"source":["features_list = []\n","for signal in train_data:\n","    spec = extract_mel_spectrogram(signal, signal.shape[0], 0.064, 0.032)\n","    feature = concatenate_frames(spec)\n","    features_list.append(feature)"]},{"cell_type":"code","execution_count":null,"id":"1f374525-c247-40c6-96d8-944cb45fd1a1","metadata":{"id":"1f374525-c247-40c6-96d8-944cb45fd1a1"},"outputs":[],"source":["base_directory = 'path/to/dataset'\n","source_test_audio_files = load_audio_files(base_directory, dataset='source_test')\n","target_test_audio_files = load_audio_files(base_directory, dataset='target_test')\n","\n","test_audio_files = source_test_audio_files + target_test_audio_files\n","test_data = np.array([x[0] for x in test_audio_files])\n","test_machine = [x[2] for x in test_audio_files]\n","test_source = [x[3] for x in test_audio_files]\n","test_label = [x[4] for x in test_audio_files]\n","test_section = [x[5] for x in test_audio_files]\n","test_labels = [(a, b, c, d) for a, b, c, d in zip(test_machine, test_source, test_label, test_section)]\n","\n","features_list = []\n","for signal in test_data:\n","    spec = extract_mel_spectrogram(signal, signal.shape[0], 0.064, 0.032)\n","    feature = concatenate_frames(spec)\n","    features_list.append(feature)\n","\n","pickle_data(features_list, test_labels, 'test')"]},{"cell_type":"code","execution_count":null,"id":"87ad3e59-31cc-4535-bba5-5206d01a7c3c","metadata":{"id":"87ad3e59-31cc-4535-bba5-5206d01a7c3c"},"outputs":[],"source":["# import numpy as np\n","from sklearn.preprocessing import PowerTransformer\n","\n","# Reshape data to 2D\n","train_data = np.array(features_list)\n","train_data_old = train_data\n","data = train_data\n","batch_size, height, width = data.shape\n","data_reshaped = data.reshape(batch_size, -1)  # Shape becomes (1500, 28*640)\n","\n","# Initialize the PowerTransformer\n","pt = PowerTransformer(method='yeo-johnson', standardize=True)\n","# scaler = StandardScaler()\n","\n","# Apply the transform\n","data_transformed = pt.fit_transform(data_reshaped)\n","\n","# Reshape back to the original 3D shape\n","data_transformed_reshaped = data_transformed.reshape(batch_size, height, width)\n","\n","# Print shapes to verify\n","print(\"Original shape:\", data.shape)\n","print(\"Reshaped for transformation:\", data_reshaped.shape)\n","print(\"Transformed shape:\", data_transformed.shape)\n","print(\"Reshaped back to original shape:\", data_transformed_reshaped.shape)\n","\n","train_data = data_transformed_reshaped\n","\n","test_data = np.array(features_list)\n","test_data_old = test_data\n","data = test_data\n","\n","# Reshape data to 2D\n","batch_size, height, width = data.shape\n","data_reshaped = data.reshape(batch_size, -1)  # Shape becomes (1500, 28*640)\n","\n","# Initialize the PowerTransformer\n","pt = PowerTransformer(method='yeo-johnson', standardize=True)\n","# scaler = StandardScaler()\n","\n","# Apply the transform\n","data_transformed = pt.fit_transform(data_reshaped)\n","\n","# Reshape back to the original 3D shape\n","data_transformed_reshaped = data_transformed.reshape(batch_size, height, width)\n","\n","# Print shapes to verify\n","print(\"Original shape:\", data.shape)\n","print(\"Reshaped for transformation:\", data_reshaped.shape)\n","print(\"Transformed shape:\", data_transformed.shape)\n","print(\"Reshaped back to original shape:\", data_transformed_reshaped.shape)\n","\n","test_data = data_transformed_reshaped"]},{"cell_type":"code","execution_count":null,"id":"77761aca-bda5-476b-819f-8fd74b1dae53","metadata":{"id":"77761aca-bda5-476b-819f-8fd74b1dae53"},"outputs":[],"source":["def pickle_data(features_arr, label_lst, filename1 ):\n","    import pickle\n","    # features_arr = np.array(feat_list)\n","    labels = label_lst\n","    filename_data = f'{filename1}_data_pt.pkl'\n","    filename_label = f'{filename1}_label_pt.pkl'\n","    with open(filename_data, 'wb') as file1:\n","        pickle.dump(features_arr, file1)\n","    with open(filename_label, 'wb') as file2:\n","        pickle.dump(labels, file2)\n","\n","pickle_data(train_data, train_labels, 'train')"]},{"cell_type":"code","execution_count":null,"id":"27a9a808-4d65-409d-a3f1-86a0a9de879e","metadata":{"id":"27a9a808-4d65-409d-a3f1-86a0a9de879e"},"outputs":[],"source":["# get pretrained embeddings from raw waveform\n","\n","model = openl3.models.load_audio_embedding_model(input_repr='mel128', content_type='env',\n","                                                 embedding_size=512, frontend='librosa')\n","\n","\n","def pickle_emb_batch(feat_list, filename):\n","    import pickle\n","    feat_arr = np.array(feat_list)\n","    filepath = f\"{filename}.pkl\"\n","    with open(filepath, 'wb') as file:\n","        pickle.dump(feat_arr, file)"]},{"cell_type":"code","source":["emb_list = list()\n","ts_list = list()\n","\n","for i, audio in enumerate(test_data_waveform):\n","    # print(i, audio.shape)\n","\n","    emb, ts = openl3.get_audio_embedding(audio, 16000, model=model,\n","                                         input_repr='mel128', frontend='librosa',\n","                                         verbose=1)\n","    emb_list.append(emb)\n","    ts_list.append(ts)\n","\n","    if i % 500 == 0:\n","        print(f'{i} processed')\n","        if i !=0:\n","            try:\n","                pickle_emb_batch(emb_list, f'emb_valve_test_batch{i+1}')\n","            except:\n","                pass\n"],"metadata":{"id":"pirVraOuSyJ7"},"id":"pirVraOuSyJ7","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ShteMF6cSyLO"},"id":"ShteMF6cSyLO","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}